{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "reaststroke': 4701, 'breeze': 4702, 'bridesmaid': 4703, 'briefcases': 4704, 'bring': 4705, 'broccoli': 4706, 'brochures': 4707, 'brook': 4708, 'brothers': 4709, 'brownish': 4710, 'buck': 4711, 'bucked': 4712, 'buddhist': 4713, 'budweiser': 4714, 'buffalo': 4715, 'bugs': 4716, 'builder': 4717, 'builds': 4718, 'bulldog': 4719, 'bullet': 4720, 'bump': 4721, 'bun': 4722, 'bunches': 4723, 'bundles': 4724, 'bunk': 4725, 'burying': 4726, 'bushel': 4727, 'busily': 4728, 'busker': 4729, 'busking': 4730, 'bust': 4731, 'buttons': 4732, 'buys': 4733, 'bystander': 4734, 'cadillac': 4735, 'caged': 4736, 'calendar': 4737, 'calmly': 4738, 'calvin': 4739, 'camo': 4740, 'camps': 4741, 'campus': 4742, 'candlelight': 4743, 'canoeing': 4744, 'capri': 4745, 'capris': 4746, 'captivated': 4747, 'capture': 4748, 'caravan': 4749, 'caressing': 4750, 'carol': 4751, 'carpenter': 4752, 'cartons': 4753, 'cashier': 4754, 'castro': 4755, 'catamaran': 4756, 'cavern': 4757, 'cell': 4758, 'cells': 4759, 'cellular': 4760, 'central': 4761, 'challenging': 4762, 'cheered': 4763, 'cheerful': 4764, 'chemical': 4765, 'chemistry': 4766, 'chief': 4767, 'chili': 4768, 'chimney': 4769, 'chipper': 4770, 'chipping': 4771, 'chisel': 4772, 'chisels': 4773, 'chopped': 4774, 'choppy': 4775, 'choreographed': 4776, 'chores': 4777, 'churning': 4778, 'circles': 4779, 'circuit': 4780, 'circuits': 4781, 'citizen': 4782, 'civilians': 4783, 'clasps': 4784, 'classical': 4785, 'clause': 4786, 'clenches': 4787, 'clever': 4788, 'clicking': 4789, 'cliffs': 4790, 'climbed': 4791, 'clipboard': 4792, 'clips': 4793, 'clog': 4794, 'closing': 4795, 'clustered': 4796, 'coached': 4797, 'coaches': 4798, 'coastline': 4799, 'coated': 4800, 'cob': 4801, 'coconut': 4802, 'coeds': 4803, 'colleagues': 4804, 'collision': 4805, 'colonial': 4806, 'colourful': 4807, 'comforter': 4808, 'comforts': 4809, 'comic': 4810, 'comical': 4811, 'common': 4812, 'compartment': 4813, 'competitively': 4814, 'completes': 4815, 'concentrating': 4816, 'concession': 4817, 'condoms': 4818, 'congregation': 4819, 'consoles': 4820, 'consulting': 4821, 'consults': 4822, 'contained': 4823, 'containers': 4824, 'contestant': 4825, 'contestants': 4826, 'contested': 4827, 'contingent': 4828, 'contorts': 4829, 'controlled': 4830, 'converses': 4831, 'conveyor': 4832, 'copies': 4833, 'copy': 4834, 'cork': 4835, 'corsage': 4836, 'cottage': 4837, 'couches': 4838, 'could': 4839, 'counters': 4840, 'coworker': 4841, 'crabs': 4842, 'cracked': 4843, 'crag': 4844, 'crammed': 4845, 'cramped': 4846, 'crayola': 4847, 'creation': 4848, 'creature': 4849, 'creepy': 4850, 'crow': 4851, 'crumbling': 4852, 'crushed': 4853, 'cue': 4854, 'culinary': 4855, 'cultural': 4856, 'cupping': 4857, 'curbside': 4858, 'curiously': 4859, 'custody': 4860, 'cutoff': 4861, 'cutouts': 4862, 'cutter': 4863, 'cyclone': 4864, 'dachshund': 4865, 'dame': 4866, 'dandelion': 4867, 'dane': 4868, 'dappled': 4869, 'daring': 4870, 'darth': 4871, 'dash': 4872, 'david': 4873, 'de': 4874, 'decides': 4875, 'deciding': 4876, 'decked': 4877, 'defending': 4878, 'deflated': 4879, 'delight': 4880, 'delivering': 4881, 'delivery': 4882, 'dental': 4883, 'depicted': 4884, 'describing': 4885, 'designing': 4886, 'detail': 4887, 'detergent': 4888, 'determined': 4889, 'devil': 4890, 'diamond': 4891, 'diaper': 4892, 'die': 4893, 'diet': 4894, 'dim': 4895, 'dingy': 4896, 'dip': 4897, 'direct': 4898, 'disco': 4899, 'discovering': 4900, 'discusses': 4901, 'distracted': 4902, 'district': 4903, 'documents': 4904, 'dollhouse': 4905, 'donations': 4906, 'doorstep': 4907, 'downing': 4908, 'downward': 4909, 'downwards': 4910, 'dragster': 4911, 'drain': 4912, 'drape': 4913, 'drawer': 4914, 'drawers': 4915, 'drawings': 4916, 'dribble': 4917, 'drifts': 4918, 'drummers': 4919, 'drumming': 4920, 'drunken': 4921, 'dueling': 4922, 'dumping': 4923, 'dutch': 4924, 'eagle': 4925, 'easy': 4926, 'eclectic': 4927, 'ecstatically': 4928, 'egret': 4929, 'eiffel': 4930, 'elaborately': 4931, 'elegant': 4932, 'eleven': 4933, 'em': 4934, 'embraces': 4935, 'emotion': 4936, 'empanadas': 4937, 'emptying': 4938, 'ended': 4939, 'engaging': 4940, 'england': 4941, 'enjoyed': 4942, 'enrolling': 4943, 'ensemble': 4944, 'ensure': 4945, 'entertainers': 4946, 'enthusiastic': 4947, 'envelope': 4948, 'erected': 4949, 'escalators': 4950, 'espresso': 4951, 'ethnicities': 4952, 'evenly': 4953, 'exam': 4954, 'examined': 4955, 'executes': 4956, 'exercises': 4957, 'exotic': 4958, 'experiments': 4959, 'explains': 4960, 'express': 4961, 'exterior': 4962, 'eyed': 4963, 'eyeliner': 4964, 'f': 4965, 'fairway': 4966, 'falcon': 4967, 'famous': 4968, 'fanny': 4969, 'fare': 4970, 'farming': 4971, 'fatigued': 4972, 'fatigues': 4973, 'faux': 4974, 'feels': 4975, 'fencers': 4976, 'ferns': 4977, 'ferris': 4978, 'fest': 4979, 'fetches': 4980, 'fiddles': 4981, 'fierce': 4982, 'figurines': 4983, 'filed': 4984, 'filmed': 4985, 'fingernails': 4986, 'finishes': 4987, 'fired': 4988, 'fires': 4989, 'fitting': 4990, 'fixture': 4991, 'flamboyant': 4992, 'flame': 4993, 'flap': 4994, 'flaps': 4995, 'flash': 4996, 'flashy': 4997, 'flexible': 4998, 'flexing': 4999, 'flier': 5000, 'flinging': 5001, 'flipped': 5002, 'flow': 5003, 'fluid': 5004, 'flutes': 5005, 'focal': 5006, 'folders': 5007, 'forearm': 5008, 'forehand': 5009, 'forever': 5010, 'formally': 5011, 'fours': 5012, 'francisco': 5013, 'frantically': 5014, 'freestyle': 5015, 'freight': 5016, 'frilly': 5017, 'fringed': 5018, 'frolic': 5019, 'frolics': 5020, 'fronds': 5021, 'frothy': 5022, 'frown': 5023, 'frowning': 5024, 'frowns': 5025, 'function': 5026, 'fundraiser': 5027, 'funky': 5028, 'furiously': 5029, 'gallon': 5030, 'gambling': 5031, 'garish': 5032, 'garments': 5033, 'garter': 5034, 'gates': 5035, 'gateway': 5036, 'gig': 5037, 'ginger': 5038, 'gingerbread': 5039, 'glances': 5040, 'glancing': 5041, 'glassy': 5042, 'glides': 5043, 'glittery': 5044, 'glow': 5045, 'glowing': 5046, 'glue': 5047, 'god': 5048, 'golfers': 5049, 'goofy': 5050, 'graffited': 5051, 'grain': 5052, 'grains': 5053, 'grandfather': 5054, 'grandparents': 5055, 'grasps': 5056, 'grate': 5057, 'graze': 5058, 'greenhouse': 5059, 'grilled': 5060, 'grin': 5061, 'grind': 5062, 'grinder': 5063, 'grinning': 5064, 'grips': 5065, 'groucho': 5066, 'grounds': 5067, 'growling': 5068, 'guidebook': 5069, 'guides': 5070, 'guiding': 5071, 'hairdo': 5072, 'hairdresser': 5073, 'hairy': 5074, 'halfway': 5075, 'hamming': 5076, 'hamper': 5077, 'handbags': 5078, 'handgun': 5079, 'handhold': 5080, 'handkerchiefs': 5081, 'handled': 5082, 'handler': 5083, 'handshake': 5084, 'hardware': 5085, 'harlem': 5086, 'harvest': 5087, 'harvested': 5088, 'hatchet': 5089, 'haul': 5090, 'hazard': 5091, 'headdresses': 5092, 'header': 5093, 'headfirst': 5094, 'headlight': 5095, 'heat': 5096, 'hedges': 5097, 'height': 5098, 'heineken': 5099, 'helper': 5100, 'herbs': 5101, 'herd': 5102, 'heritage': 5103, 'heron': 5104, 'hid': 5105, 'hide': 5106, 'hides': 5107, 'hijab': 5108, 'hip': 5109, 'hippie': 5110, 'hipsters': 5111, 'historical': 5112, 'hitchhiking': 5113, 'hitching': 5114, 'hitter': 5115, 'hoe': 5116, 'hog': 5117, 'hokey': 5118, 'hollow': 5119, 'homes': 5120, 'homework': 5121, 'hoodies': 5122, 'hooked': 5123, 'hoping': 5124, 'hopper': 5125, 'hops': 5126, 'horizontal': 5127, 'horsemen': 5128, 'household': 5129, 'houston': 5130, 'hovering': 5131, 'hugged': 5132, 'hulk': 5133, 'humans': 5134, 'humping': 5135, 'hungry': 5136, 'hunt': 5137, 'hunter': 5138, 'hurry': 5139, 'hurt': 5140, 'hydraulic': 5141, 'iced': 5142, 'idle': 5143, 'igloo': 5144, 'illuminating': 5145, 'imitation': 5146, 'immigration': 5147, 'important': 5148, 'improvement': 5149, 'incense': 5150, 'inclined': 5151, 'incredible': 5152, 'indigenous': 5153, 'inflating': 5154, 'info': 5155, 'informally': 5156, 'ink': 5157, 'innertube': 5158, 'insect': 5159, 'inspectors': 5160, 'inspired': 5161, 'instructions': 5162, 'instructors': 5163, 'international': 5164, 'internet': 5165, 'interviews': 5166, 'intricate': 5167, 'intrigued': 5168, 'involves': 5169, 'iowa': 5170, 'irish': 5171, 'irons': 5172, 'irritated': 5173, 'islamic': 5174, 'isle': 5175, 'issue': 5176, 'italy': 5177, 'jackhammers': 5178, 'jacks': 5179, 'jackson': 5180, 'jagged': 5181, 'james': 5182, 'jesus': 5183, 'jets': 5184, 'jim': 5185, 'johns': 5186, 'joking': 5187, 'jostle': 5188, 'journal': 5189, 'jousting': 5190, 'judge': 5191, 'juggle': 5192, 'juggler': 5193, 'kawasaki': 5194, 'kelp': 5195, 'kerchief': 5196, 'kickboxer': 5197, 'kill': 5198, 'kimono': 5199, 'king': 5200, 'klein': 5201, 'knitted': 5202, 'knock': 5203, 'knotted': 5204, 'kwon': 5205, 'labor': 5206, 'labradoodle': 5207, 'lace': 5208, 'lack': 5209, 'language': 5210, 'laps': 5211, 'lapse': 5212, 'las': 5213, 'later': 5214, 'lava': 5215, 'lavender': 5216, 'le': 5217, 'leader': 5218, 'leak': 5219, 'leaped': 5220, 'learn': 5221, 'learns': 5222, 'leashed': 5223, 'leave': 5224, 'lebron': 5225, 'leisurely': 5226, 'lemonade': 5227, 'lettering': 5228, 'letting': 5229, 'lettuce': 5230, 'licked': 5231, 'lighthouse': 5232, 'likes': 5233, 'limbs': 5234, 'linens': 5235, 'list': 5236, 'lizard': 5237, 'loader': 5238, 'loaf': 5239, 'locker': 5240, 'lockers': 5241, 'locomotive': 5242, 'locomotives': 5243, 'lodge': 5244, 'los': 5245, 'loses': 5246, 'loudspeaker': 5247, 'louvre': 5248, 'lovers': 5249, 'lowering': 5250, 'magic': 5251, 'magician': 5252, 'mail': 5253, 'mainly': 5254, 'maintain': 5255, 'maintained': 5256, 'mallet': 5257, 'management': 5258, 'mane': 5259, 'manicured': 5260, 'manner': 5261, 'mantle': 5262, 'manure': 5263, 'maracas': 5264, 'marshal': 5265, 'mart': 5266, 'martini': 5267, 'marx': 5268, 'masonry': 5269, 'mass': 5270, 'massage': 5271, 'masses': 5272, 'masterpiece': 5273, 'matador': 5274, 'matches': 5275, 'math': 5276, 'mattresses': 5277, 'mauve': 5278, 'meats': 5279, 'medals': 5280, 'median': 5281, 'melon': 5282, 'melons': 5283, 'memorabilia': 5284, 'messages': 5285, 'met': 5286, 'metalworking': 5287, 'meter': 5288, 'michael': 5289, 'michigan': 5290, 'milling': 5291, 'mime': 5292, 'mine': 5293, 'miners': 5294, 'mingle': 5295, 'mingling': 5296, 'minnesota': 5297, 'mirrors': 5298, 'miscellaneous': 5299, 'missed': 5300, 'misty': 5301, 'mixture': 5302, 'mma': 5303, 'mob': 5304, 'modeling': 5305, 'mold': 5306, 'moms': 5307, 'mongolian': 5308, 'mood': 5309, 'moon': 5310, 'moored': 5311, 'moss': 5312, 'motions': 5313, 'mounds': 5314, 'mount': 5315, 'mover': 5316, 'mowed': 5317, 'mr.': 5318, 'mrs.': 5319, 'mugs': 5320, 'mulch': 5321, 'multitasking': 5322, 'murals': 5323, 'mushroom': 5324, 'mustang': 5325, 'nanny': 5326, 'natives': 5327, 'navigate': 5328, 'nears': 5329, 'neatly': 5330, 'necks': 5331, 'necktie': 5332, 'need': 5333, 'needle': 5334, 'neighbors': 5335, 'nerf': 5336, 'nibbles': 5337, 'ninja': 5338, 'nips': 5339, 'non': 5340, 'normal': 5341, 'notre': 5342, 'nuts': 5343, 'oakland': 5344, 'oars': 5345, 'oblivious': 5346, 'occasion': 5347, 'occurred': 5348, 'oceanside': 5349, 'oddly': 5350, 'offer': 5351, 'offered': 5352, 'officials': 5353, 'oh': 5354, 'olympians': 5355, 'oncoming': 5356, 'op': 5357, 'openings': 5358, 'operate': 5359, 'opinions': 5360, 'origin': 5361, 'orioles': 5362, 'ornamental': 5363, 'outboard': 5364, 'overcast': 5365, 'owl': 5366, 'owned': 5367, 'ox': 5368, 'oxygen': 5369, 'p': 5370, 'pabst': 5371, 'packs': 5372, 'paddled': 5373, 'paddy': 5374, 'page': 5375, 'pajama': 5376, 'palace': 5377, 'pancakes': 5378, 'pane': 5379, 'paperback': 5380, 'para': 5381, 'parachutist': 5382, 'paraphernalia': 5383, 'parasail': 5384, 'paris': 5385, 'parks': 5386, 'parrot': 5387, 'participant': 5388, 'particular': 5389, 'passer': 5390, 'pasture': 5391, 'patchy': 5392, 'patrick': 5393, 'patrolling': 5394, 'patties': 5395, 'pearls': 5396, 'pebbles': 5397, 'pedals': 5398, 'peddling': 5399, 'pee': 5400, 'peek': 5401, 'peep': 5402, 'pelican': 5403, 'penguins': 5404, 'penn': 5405, 'penske': 5406, 'peoples': 5407, 'peppers': 5408, 'percussion': 5409, 'peruses': 5410, 'pf': 5411, 'pharmacy': 5412, 'physically': 5413, 'pies': 5414, 'pigs': 5415, 'pineapple': 5416, 'pineapples': 5417, 'pioneer': 5418, 'pitbull': 5419, 'placard': 5420, 'placid': 5421, 'plan': 5422, 'planes': 5423, 'plaque': 5424, 'plaster': 5425, 'platforms': 5426, 'platter': 5427, 'plow': 5428, 'plume': 5429, 'plywood': 5430, 'podiums': 5431, 'pokemon': 5432, 'pollution': 5433, 'pomeranian': 5434, 'pompoms': 5435, 'popsicle': 5436, 'populated': 5437, 'pork': 5438, 'possible': 5439, 'postcards': 5440, 'poultry': 5441, 'pounces': 5442, 'pouncing': 5443, 'pout': 5444, 'powdered': 5445, 'pray': 5446, 'prepping': 5447, 'preschool': 5448, 'presented': 5449, 'presumably': 5450, 'pretend': 5451, 'pretzels': 5452, 'previously': 5453, 'price': 5454, 'private': 5455, 'proclaim': 5456, 'programs': 5457, 'projection': 5458, 'prop': 5459, 'propeller': 5460, 'props': 5461, 'protectors': 5462, 'puff': 5463, 'puma': 5464, 'punches': 5465, 'putt': 5466, 'puzzled': 5467, 'q': 5468, 'quartet': 5469, 'question': 5470, 'questions': 5471, 'quietly': 5472, 'quilted': 5473, 'rackets': 5474, 'racks': 5475, 'rad': 5476, 'raging': 5477, 'ran': 5478, 'random': 5479, 'rates': 5480, 'rather': 5481, 'raw': 5482, 'reacting': 5483, 'reaction': 5484, 'reason': 5485, 'received': 5486, 'recent': 5487, 'reclining': 5488, 'recorder': 5489, 'recycling': 5490, 'reenactment': 5491, 'reflecting': 5492, 'reflects': 5493, 'refreshments': 5494, 'refuge': 5495, 'regal': 5496, 'region': 5497, 'registered': 5498, 'reindeer': 5499, 'relaxed': 5500, 'release': 5501, 'releases': 5502, 'releasing': 5503, 'reluctant': 5504, 'remember': 5505, 'repairman': 5506, 'repelling': 5507, 'reporters': 5508, 'researchers': 5509, 'residence': 5510, 'residential': 5511, 'respective': 5512, 'respectively': 5513, 'restaraunt': 5514, 'retrieve': 5515, 'ribs': 5516, 'rican': 5517, 'rid': 5518, 'rifles': 5519, 'rinsing': 5520, 'rising': 5521, 'risk': 5522, 'riverbed': 5523, 'rocker': 5524, 'rocket': 5525, 'rodgers': 5526, 'role': 5527, 'roots': 5528, 'rose': 5529, 'routines': 5530, 'rugged': 5531, 'rushing': 5532, 'russian': 5533, 'rust': 5534, 'rusted': 5535, 'saddled': 5536, 'safely': 5537, 'sales': 5538, 'salesman': 5539, 'saluting': 5540, 'samples': 5541, 'samsung': 5542, 'san': 5543, 'sandwiches': 5544, 'sash': 5545, 'sauce': 5546, 'saucer': 5547, 'sausage': 5548, 'saving': 5549, 'sax': 5550, 'scaffolds': 5551, 'scan': 5552, 'scary': 5553, 'schools': 5554, 'scientific': 5555, 'scooby': 5556, 'scope': 5557, 'scores': 5558, 'scoring': 5559, 'scrabble': 5560, 'scramble': 5561, 'scrapping': 5562, 'screened': 5563, 'screens': 5564, 'scrimmage': 5565, 'scrubbing': 5566, 'scruffy': 5567, 'scythe': 5568, 'seal': 5569, 'seamstress': 5570, 'seas': 5571, 'sections': 5572, 'secure': 5573, 'secures': 5574, 'securing': 5575, 'segway': 5576, 'selections': 5577, 'selects': 5578, 'seminar': 5579, 'sending': 5580, 'sends': 5581, 'separated': 5582, 'separates': 5583, 'sequence': 5584, 'serene': 5585, 'serious': 5586, 'sesame': 5587, 'setup': 5588, 'sew': 5589, 'sewer': 5590, 'sews': 5591, 'shabby': 5592, 'shacking': 5593, 'shadowy': 5594, 'shampoo': 5595, 'shawls': 5596, 'shell': 5597, 'shepard': 5598, 'shift': 5599, 'shine': 5600, 'shined': 5601, 'shipping': 5602, 'shish': 5603, 'shoelaces': 5604, 'shots': 5605, 'should': 5606, 'shouts': 5607, 'showcasing': 5608, 'showers': 5609, 'showgirl': 5610, 'shrine': 5611, 'shrubbery': 5612, 'shuffles': 5613, 'sibling': 5614, 'sideline': 5615, 'sidewalks': 5616, 'silverware': 5617, 'simple': 5618, 'simply': 5619, 'simultaneously': 5620, 'sisters': 5621, 'sites': 5622, 'situation': 5623, 'size': 5624, 'sketch': 5625, 'skiiers': 5626, 'skim': 5627, 'skit': 5628, 'skulls': 5629, 'skyscraper': 5630, 'slalom': 5631, 'slamming': 5632, 'sledgehammer': 5633, 'sleepy': 5634, 'sliced': 5635, 'slim': 5636, 'sling': 5637, 'slingshot': 5638, 'slips': 5639, 'slow': 5640, 'slumped': 5641, 'slung': 5642, 'smear': 5643, 'smelling': 5644, 'smock': 5645, 'snail': 5646, 'snarling': 5647, 'snoopy': 5648, 'snorkeling': 5649, 'snowdrift': 5650, 'snowflakes': 5651, 'snowmobile': 5652, 'snowmobiling': 5653, 'snowshoe': 5654, 'snowstorm': 5655, 'snowsuits': 5656, 'snuggling': 5657, 'soaked': 5658, 'soaking': 5659, 'soapy': 5660, 'sodas': 5661, 'softly': 5662, 'solid': 5663, 'source': 5664, 'sous': 5665, 'southeast': 5666, 'spa': 5667, 'spaghetti': 5668, 'spare': 5669, 'sparsely': 5670, 'spattered': 5671, 'specialized': 5672, 'specific': 5673, 'speckled': 5674, 'spend': 5675, 'spending': 5676, 'spiky': 5677, 'spinner': 5678, 'spirit': 5679, 'splits': 5680, 'spoons': 5681, 'spotlights': 5682, 'spout': 5683, 'squash': 5684, 'squirrel': 5685, 'squirted': 5686, 'squirts': 5687, 'stainless': 5688, 'stake': 5689, 'starfish': 5690, 'started': 5691, 'steal': 5692, 'steals': 5693, 'steeplechase': 5694, 'steering': 5695, 'stem': 5696, 'stitch': 5697, 'stock': 5698, 'stoops': 5699, 'stopping': 5700, 'stories': 5701, 'stout': 5702, 'streamers': 5703, 'streaming': 5704, 'stricken': 5705, 'strips': 5706, 'struggle': 5707, 'strums': 5708, 'strung': 5709, 'stumbling': 5710, 'stunning': 5711, 'styled': 5712, 'styling': 5713, 'stylist': 5714, 'subject': 5715, 'submerges': 5716, 'sucker': 5717, 'sunday': 5718, 'sunflower': 5719, 'super': 5720, 'superior': 5721, 'supervision': 5722, 'supported': 5723, 'supports': 5724, 'surfboarder': 5725, 'surprised': 5726, 'sushi': 5727, 'suspect': 5728, 'suspension': 5729, 'sveral': 5730, 'swaddled': 5731, 'swallowing': 5732, 'swamp': 5733, 'swans': 5734, 'sweat': 5735, 'sweatpants': 5736, 'sweets': 5737, 'swerves': 5738, 'swoops': 5739, 'swung': 5740, 'symphony': 5741, 'syrup': 5742, 'tackler': 5743, 'tae': 5744, 'tambourines': 5745, 'taped': 5746, 'taping': 5747, 'targets': 5748, 'teachers': 5749, 'teaches': 5750, 'tear': 5751, 'tears': 5752, 'tech': 5753, 'technical': 5754, 'technician': 5755, 'teeter': 5756, 'tell': 5757, 'terminals': 5758, 'tests': 5759, 'textile': 5760, 'thai': 5761, 'thatched': 5762, 'thing': 5763, 'thomas': 5764, 'thong': 5765, 'thorugh': 5766, 'thousands': 5767, 'throat': 5768, 'throughout': 5769, 'thursday': 5770, 'tilted': 5771, 'tilts': 5772, 'tim': 5773, 'timber': 5774, 'tin': 5775, 'tinsel': 5776, 'tinted': 5777, 'tipped': 5778, 'tipping': 5779, 'toenails': 5780, 'toes': 5781, 'toil': 5782, 'toll': 5783, 'tongues': 5784, 'tortillas': 5785, 'tote': 5786, 'toting': 5787, 'trade': 5788, 'trading': 5789, 'trails': 5790, 'transformer': 5791, 'travelling': 5792, 'traverse': 5793, 'treads': 5794, 'treatment': 5795, 'treed': 5796, 'treeless': 5797, 'trekking': 5798, 'trendy': 5799, 'tress': 5800, 'trim': 5801, 'trinkets': 5802, 'troop': 5803, 'trooper': 5804, 'true': 5805, 'tucked': 5806, 'tugs': 5807, 'turbulent': 5808, 'turf': 5809, 'turkeys': 5810, 'tux': 5811, 'twenty': 5812, 'twin': 5813, 'twins': 5814, 'twos': 5815, 'u.s.': 5816, 'ufc': 5817, 'underside': 5818, 'unfriendly': 5819, 'unkempt': 5820, 'unload': 5821, 'unwraps': 5822, 'upraised': 5823, 'upstairs': 5824, 'upturned': 5825, 'urinal': 5826, 'v': 5827, 'varied': 5828, 'vase': 5829, 'vault': 5830, 'version': 5831, 'vessel': 5832, 'victim': 5833, 'viewed': 5834, 'vikings': 5835, 'vine': 5836, 'violence': 5837, 'violet': 5838, 'vision': 5839, 'visit': 5840, 'visitors': 5841, 'voice': 5842, 'volcano': 5843, 'volunteer': 5844, 'vw': 5845, 'waffle': 5846, 'wagons': 5847, 'wakeboarding': 5848, 'wakeboards': 5849, 'walkman': 5850, 'wand': 5851, 'wands': 5852, 'wanting': 5853, 'wants': 5854, 'warmers': 5855, 'washers': 5856, 'watered': 5857, 'waterfalls': 5858, 'waterproof': 5859, 'watery': 5860, 'wavy': 5861, 'weights': 5862, 'welder': 5863, 'wells': 5864, 'wetland': 5865, 'whatever': 5866, 'whisking': 5867, 'wielding': 5868, 'wildfire': 5869, 'windowsill': 5870, 'winds': 5871, 'windsurfer': 5872, 'wineglass': 5873, 'winks': 5874, 'wits': 5875, 'wok': 5876, 'wondering': 5877, 'worked': 5878, 'worn': 5879, 'worried': 5880, 'would': 5881, 'wounds': 5882, 'wrangle': 5883, 'wrench': 5884, 'wrestles': 5885, 'wristband': 5886, 'wrists': 5887, 'yells': 5888, 'ymca': 5889, 'zigzag': 5890, 'zooms': 5891, 'zune': 5892})\n"
    }
   ],
   "source": [
    "print(TRG.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: int,\n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "\n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "\n",
    "        return weighted_encoder_rep\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input: Tensor,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_encoder_rep,\n",
    "                                     embedded), dim = 1))\n",
    "\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The model has 1,856,653 trainable parameters\n"
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 01 | Time: 5m 49s\n\tTrain Loss: 5.684 | Train PPL: 294.040\n\t Val. Loss: 5.245 |  Val. PPL: 189.564\nEpoch: 02 | Time: 5m 39s\n\tTrain Loss: 5.000 | Train PPL: 148.456\n\t Val. Loss: 5.102 |  Val. PPL: 164.325\nEpoch: 03 | Time: 5m 37s\n\tTrain Loss: 4.751 | Train PPL: 115.708\n\t Val. Loss: 4.992 |  Val. PPL: 147.176\nEpoch: 04 | Time: 5m 40s\n\tTrain Loss: 4.586 | Train PPL:  98.125\n\t Val. Loss: 4.871 |  Val. PPL: 130.387\nEpoch: 05 | Time: 5m 36s\n\tTrain Loss: 4.428 | Train PPL:  83.743\n\t Val. Loss: 4.851 |  Val. PPL: 127.881\nEpoch: 06 | Time: 5m 33s\n\tTrain Loss: 4.336 | Train PPL:  76.409\n\t Val. Loss: 4.904 |  Val. PPL: 134.827\nEpoch: 07 | Time: 5m 33s\n\tTrain Loss: 4.214 | Train PPL:  67.602\n\t Val. Loss: 4.760 |  Val. PPL: 116.729\nEpoch: 08 | Time: 5m 36s\n\tTrain Loss: 4.126 | Train PPL:  61.930\n\t Val. Loss: 4.680 |  Val. PPL: 107.815\nEpoch: 09 | Time: 5m 36s\n\tTrain Loss: 4.020 | Train PPL:  55.704\n\t Val. Loss: 4.594 |  Val. PPL:  98.848\nEpoch: 10 | Time: 5m 38s\n\tTrain Loss: 3.939 | Train PPL:  51.364\n\t Val. Loss: 4.521 |  Val. PPL:  91.891\n| Test Loss: 4.529 | Test PPL:  92.624 |\n"
    }
   ],
   "source": [
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ]
}