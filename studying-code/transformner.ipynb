{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import copy\n",
    "from typing import Optional, Any\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2, att_weights = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src, att_weights\n",
    "\n",
    "class TransformerEncoder(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "\n",
    "    Args:\n",
    "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = transformer_encoder(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        att = []\n",
    "        for mod in self.layers:\n",
    "            output, att_weights = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "            att.append(att_weights.data)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output, att = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 20\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 15\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 3 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, _ = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "def test_sample(eval_model):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        sentence = torch.tensor([[TEXT.vocab.stoi['i']], [TEXT.vocab.stoi['am']], [TEXT.vocab.stoi['a']], [TEXT.vocab.stoi['student']], [TEXT.vocab.stoi['studying']],          [TEXT.vocab.stoi['at']], [TEXT.vocab.stoi['a']], [TEXT.vocab.stoi['high']], [TEXT.vocab.stoi['school']], [TEXT.vocab.stoi['.']]]).to(device)\n",
    "        _, att = eval_model(sentence)\n",
    "    return att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hes | lr 0.04 | ms/batch 14.95 | loss  4.27 | ppl    71.72\n-----------------------------------------------------------------------------------------\n| end of epoch  94 | time: 106.06s | valid loss  5.36 | valid ppl   211.91\n-----------------------------------------------------------------------------------------\n| epoch  95 |   200/ 6955 batches | lr 0.04 | ms/batch 14.93 | loss  4.38 | ppl    79.68\n| epoch  95 |   400/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.37 | ppl    78.66\n| epoch  95 |   600/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.42 | ppl    83.50\n| epoch  95 |   800/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.41 | ppl    82.20\n| epoch  95 |  1000/ 6955 batches | lr 0.04 | ms/batch 15.02 | loss  4.35 | ppl    77.46\n| epoch  95 |  1200/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.27 | ppl    71.62\n| epoch  95 |  1400/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.23 | ppl    68.78\n| epoch  95 |  1600/ 6955 batches | lr 0.04 | ms/batch 14.92 | loss  4.28 | ppl    72.20\n| epoch  95 |  1800/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.33 | ppl    75.97\n| epoch  95 |  2000/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.39 | ppl    80.52\n| epoch  95 |  2200/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.32 | ppl    75.08\n| epoch  95 |  2400/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.31 | ppl    74.70\n| epoch  95 |  2600/ 6955 batches | lr 0.04 | ms/batch 14.94 | loss  4.32 | ppl    75.51\n| epoch  95 |  2800/ 6955 batches | lr 0.04 | ms/batch 15.00 | loss  4.37 | ppl    79.11\n| epoch  95 |  3000/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.43 | ppl    83.80\n| epoch  95 |  3200/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.34 | ppl    76.55\n| epoch  95 |  3400/ 6955 batches | lr 0.04 | ms/batch 14.85 | loss  4.34 | ppl    76.89\n| epoch  95 |  3600/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.43 | ppl    83.92\n| epoch  95 |  3800/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.39 | ppl    80.60\n| epoch  95 |  4000/ 6955 batches | lr 0.04 | ms/batch 15.00 | loss  4.34 | ppl    76.82\n| epoch  95 |  4200/ 6955 batches | lr 0.04 | ms/batch 15.03 | loss  4.41 | ppl    82.57\n| epoch  95 |  4400/ 6955 batches | lr 0.04 | ms/batch 15.00 | loss  4.43 | ppl    83.84\n| epoch  95 |  4600/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.36 | ppl    78.42\n| epoch  95 |  4800/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.34 | ppl    76.54\n| epoch  95 |  5000/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.22 | ppl    67.75\n| epoch  95 |  5200/ 6955 batches | lr 0.04 | ms/batch 15.02 | loss  4.23 | ppl    69.01\n| epoch  95 |  5400/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.31 | ppl    74.49\n| epoch  95 |  5600/ 6955 batches | lr 0.04 | ms/batch 15.01 | loss  4.36 | ppl    77.93\n| epoch  95 |  5800/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.31 | ppl    74.32\n| epoch  95 |  6000/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.36 | ppl    78.42\n| epoch  95 |  6200/ 6955 batches | lr 0.04 | ms/batch 15.00 | loss  4.36 | ppl    78.01\n| epoch  95 |  6400/ 6955 batches | lr 0.04 | ms/batch 15.02 | loss  4.28 | ppl    71.98\n| epoch  95 |  6600/ 6955 batches | lr 0.04 | ms/batch 15.00 | loss  4.32 | ppl    75.05\n| epoch  95 |  6800/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.28 | ppl    72.15\n-----------------------------------------------------------------------------------------\n| end of epoch  95 | time: 105.97s | valid loss  5.35 | valid ppl   211.35\n-----------------------------------------------------------------------------------------\n| epoch  96 |   200/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.39 | ppl    80.48\n| epoch  96 |   400/ 6955 batches | lr 0.04 | ms/batch 15.03 | loss  4.37 | ppl    79.37\n| epoch  96 |   600/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.43 | ppl    83.56\n| epoch  96 |   800/ 6955 batches | lr 0.04 | ms/batch 15.00 | loss  4.42 | ppl    82.73\n| epoch  96 |  1000/ 6955 batches | lr 0.04 | ms/batch 14.93 | loss  4.35 | ppl    77.31\n| epoch  96 |  1200/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.28 | ppl    71.96\n| epoch  96 |  1400/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.24 | ppl    69.21\n| epoch  96 |  1600/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.28 | ppl    72.20\n| epoch  96 |  1800/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.33 | ppl    76.31\n| epoch  96 |  2000/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.39 | ppl    80.36\n| epoch  96 |  2200/ 6955 batches | lr 0.04 | ms/batch 14.94 | loss  4.31 | ppl    74.75\n| epoch  96 |  2400/ 6955 batches | lr 0.04 | ms/batch 14.91 | loss  4.31 | ppl    74.63\n| epoch  96 |  2600/ 6955 batches | lr 0.04 | ms/batch 14.90 | loss  4.33 | ppl    75.65\n| epoch  96 |  2800/ 6955 batches | lr 0.04 | ms/batch 14.91 | loss  4.38 | ppl    79.55\n| epoch  96 |  3000/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.42 | ppl    83.50\n| epoch  96 |  3200/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.34 | ppl    76.71\n| epoch  96 |  3400/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.34 | ppl    77.02\n| epoch  96 |  3600/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.43 | ppl    84.31\n| epoch  96 |  3800/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.39 | ppl    80.61\n| epoch  96 |  4000/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.34 | ppl    76.49\n| epoch  96 |  4200/ 6955 batches | lr 0.04 | ms/batch 15.00 | loss  4.41 | ppl    82.26\n| epoch  96 |  4400/ 6955 batches | lr 0.04 | ms/batch 14.89 | loss  4.43 | ppl    84.00\n| epoch  96 |  4600/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.37 | ppl    78.76\n| epoch  96 |  4800/ 6955 batches | lr 0.04 | ms/batch 14.93 | loss  4.34 | ppl    76.75\n| epoch  96 |  5000/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.22 | ppl    68.34\n| epoch  96 |  5200/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.24 | ppl    69.18\n| epoch  96 |  5400/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.31 | ppl    74.73\n| epoch  96 |  5600/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.36 | ppl    78.08\n| epoch  96 |  5800/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.31 | ppl    74.23\n| epoch  96 |  6000/ 6955 batches | lr 0.04 | ms/batch 14.94 | loss  4.37 | ppl    78.78\n| epoch  96 |  6200/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.35 | ppl    77.48\n| epoch  96 |  6400/ 6955 batches | lr 0.04 | ms/batch 14.92 | loss  4.28 | ppl    71.93\n| epoch  96 |  6600/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.31 | ppl    74.61\n| epoch  96 |  6800/ 6955 batches | lr 0.04 | ms/batch 15.02 | loss  4.28 | ppl    72.16\n-----------------------------------------------------------------------------------------\n| end of epoch  96 | time: 105.87s | valid loss  5.35 | valid ppl   211.41\n-----------------------------------------------------------------------------------------\n| epoch  97 |   200/ 6955 batches | lr 0.04 | ms/batch 14.84 | loss  4.38 | ppl    79.91\n| epoch  97 |   400/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.37 | ppl    78.91\n| epoch  97 |   600/ 6955 batches | lr 0.04 | ms/batch 14.91 | loss  4.43 | ppl    83.65\n| epoch  97 |   800/ 6955 batches | lr 0.04 | ms/batch 14.94 | loss  4.42 | ppl    82.76\n| epoch  97 |  1000/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.35 | ppl    77.36\n| epoch  97 |  1200/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.28 | ppl    72.43\n| epoch  97 |  1400/ 6955 batches | lr 0.04 | ms/batch 14.94 | loss  4.23 | ppl    68.94\n| epoch  97 |  1600/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.29 | ppl    73.08\n| epoch  97 |  1800/ 6955 batches | lr 0.04 | ms/batch 15.01 | loss  4.33 | ppl    75.94\n| epoch  97 |  2000/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.39 | ppl    80.54\n| epoch  97 |  2200/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.31 | ppl    74.78\n| epoch  97 |  2400/ 6955 batches | lr 0.04 | ms/batch 15.03 | loss  4.31 | ppl    74.61\n| epoch  97 |  2600/ 6955 batches | lr 0.04 | ms/batch 14.93 | loss  4.33 | ppl    75.85\n| epoch  97 |  2800/ 6955 batches | lr 0.04 | ms/batch 14.94 | loss  4.37 | ppl    79.24\n| epoch  97 |  3000/ 6955 batches | lr 0.04 | ms/batch 14.94 | loss  4.42 | ppl    83.50\n| epoch  97 |  3200/ 6955 batches | lr 0.04 | ms/batch 15.02 | loss  4.34 | ppl    76.95\n| epoch  97 |  3400/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.35 | ppl    77.17\n| epoch  97 |  3600/ 6955 batches | lr 0.04 | ms/batch 14.93 | loss  4.43 | ppl    84.20\n| epoch  97 |  3800/ 6955 batches | lr 0.04 | ms/batch 15.01 | loss  4.40 | ppl    81.35\n| epoch  97 |  4000/ 6955 batches | lr 0.04 | ms/batch 14.95 | loss  4.34 | ppl    76.64\n| epoch  97 |  4200/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.41 | ppl    82.24\n| epoch  97 |  4400/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.44 | ppl    84.37\n| epoch  97 |  4600/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.36 | ppl    78.53\n| epoch  97 |  4800/ 6955 batches | lr 0.04 | ms/batch 15.00 | loss  4.34 | ppl    76.95\n| epoch  97 |  5000/ 6955 batches | lr 0.04 | ms/batch 14.99 | loss  4.22 | ppl    68.35\n| epoch  97 |  5200/ 6955 batches | lr 0.04 | ms/batch 15.01 | loss  4.24 | ppl    69.19\n| epoch  97 |  5400/ 6955 batches | lr 0.04 | ms/batch 15.01 | loss  4.31 | ppl    74.28\n| epoch  97 |  5600/ 6955 batches | lr 0.04 | ms/batch 14.94 | loss  4.36 | ppl    78.58\n| epoch  97 |  5800/ 6955 batches | lr 0.04 | ms/batch 14.97 | loss  4.31 | ppl    74.79\n| epoch  97 |  6000/ 6955 batches | lr 0.04 | ms/batch 14.96 | loss  4.37 | ppl    79.02\n| epoch  97 |  6200/ 6955 batches | lr 0.04 | ms/batch 14.93 | loss  4.35 | ppl    77.24\n| epoch  97 |  6400/ 6955 batches | lr 0.04 | ms/batch 15.01 | loss  4.28 | ppl    72.50\n| epoch  97 |  6600/ 6955 batches | lr 0.04 | ms/batch 14.98 | loss  4.32 | ppl    75.12\n| epoch  97 |  6800/ 6955 batches | lr 0.04 | ms/batch 15.02 | loss  4.28 | ppl    72.42\n-----------------------------------------------------------------------------------------\n| end of epoch  97 | time: 105.91s | valid loss  5.35 | valid ppl   211.17\n-----------------------------------------------------------------------------------------\n| epoch  98 |   200/ 6955 batches | lr 0.03 | ms/batch 14.95 | loss  4.38 | ppl    80.12\n| epoch  98 |   400/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.37 | ppl    78.92\n| epoch  98 |   600/ 6955 batches | lr 0.03 | ms/batch 14.90 | loss  4.43 | ppl    83.64\n| epoch  98 |   800/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.42 | ppl    83.12\n| epoch  98 |  1000/ 6955 batches | lr 0.03 | ms/batch 14.82 | loss  4.35 | ppl    77.26\n| epoch  98 |  1200/ 6955 batches | lr 0.03 | ms/batch 14.91 | loss  4.29 | ppl    73.16\n| epoch  98 |  1400/ 6955 batches | lr 0.03 | ms/batch 14.93 | loss  4.23 | ppl    68.88\n| epoch  98 |  1600/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.29 | ppl    73.20\n| epoch  98 |  1800/ 6955 batches | lr 0.03 | ms/batch 14.96 | loss  4.34 | ppl    76.81\n| epoch  98 |  2000/ 6955 batches | lr 0.03 | ms/batch 15.00 | loss  4.39 | ppl    80.54\n| epoch  98 |  2200/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.31 | ppl    74.81\n| epoch  98 |  2400/ 6955 batches | lr 0.03 | ms/batch 14.93 | loss  4.31 | ppl    74.13\n| epoch  98 |  2600/ 6955 batches | lr 0.03 | ms/batch 14.94 | loss  4.33 | ppl    76.17\n| epoch  98 |  2800/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.38 | ppl    79.65\n| epoch  98 |  3000/ 6955 batches | lr 0.03 | ms/batch 14.86 | loss  4.43 | ppl    83.73\n| epoch  98 |  3200/ 6955 batches | lr 0.03 | ms/batch 14.94 | loss  4.34 | ppl    76.97\n| epoch  98 |  3400/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.34 | ppl    76.93\n| epoch  98 |  3600/ 6955 batches | lr 0.03 | ms/batch 14.90 | loss  4.43 | ppl    84.31\n| epoch  98 |  3800/ 6955 batches | lr 0.03 | ms/batch 14.83 | loss  4.40 | ppl    81.33\n| epoch  98 |  4000/ 6955 batches | lr 0.03 | ms/batch 14.88 | loss  4.35 | ppl    77.28\n| epoch  98 |  4200/ 6955 batches | lr 0.03 | ms/batch 14.86 | loss  4.41 | ppl    82.01\n| epoch  98 |  4400/ 6955 batches | lr 0.03 | ms/batch 14.79 | loss  4.43 | ppl    84.27\n| epoch  98 |  4600/ 6955 batches | lr 0.03 | ms/batch 14.95 | loss  4.37 | ppl    78.81\n| epoch  98 |  4800/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.34 | ppl    77.05\n| epoch  98 |  5000/ 6955 batches | lr 0.03 | ms/batch 14.88 | loss  4.24 | ppl    69.12\n| epoch  98 |  5200/ 6955 batches | lr 0.03 | ms/batch 14.96 | loss  4.25 | ppl    69.81\n| epoch  98 |  5400/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.30 | ppl    73.94\n| epoch  98 |  5600/ 6955 batches | lr 0.03 | ms/batch 14.92 | loss  4.37 | ppl    79.06\n| epoch  98 |  5800/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.32 | ppl    75.09\n| epoch  98 |  6000/ 6955 batches | lr 0.03 | ms/batch 14.90 | loss  4.37 | ppl    79.35\n| epoch  98 |  6200/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.35 | ppl    77.44\n| epoch  98 |  6400/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.29 | ppl    73.00\n| epoch  98 |  6600/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.32 | ppl    75.13\n| epoch  98 |  6800/ 6955 batches | lr 0.03 | ms/batch 14.86 | loss  4.28 | ppl    72.53\n-----------------------------------------------------------------------------------------\n| end of epoch  98 | time: 105.40s | valid loss  5.35 | valid ppl   209.65\n-----------------------------------------------------------------------------------------\n| epoch  99 |   200/ 6955 batches | lr 0.03 | ms/batch 14.99 | loss  4.39 | ppl    80.30\n| epoch  99 |   400/ 6955 batches | lr 0.03 | ms/batch 15.13 | loss  4.38 | ppl    79.45\n| epoch  99 |   600/ 6955 batches | lr 0.03 | ms/batch 15.07 | loss  4.43 | ppl    83.83\n| epoch  99 |   800/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.43 | ppl    84.06\n| epoch  99 |  1000/ 6955 batches | lr 0.03 | ms/batch 15.06 | loss  4.35 | ppl    77.73\n| epoch  99 |  1200/ 6955 batches | lr 0.03 | ms/batch 15.13 | loss  4.30 | ppl    73.95\n| epoch  99 |  1400/ 6955 batches | lr 0.03 | ms/batch 15.03 | loss  4.23 | ppl    68.40\n| epoch  99 |  1600/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.30 | ppl    73.62\n| epoch  99 |  1800/ 6955 batches | lr 0.03 | ms/batch 15.08 | loss  4.34 | ppl    76.77\n| epoch  99 |  2000/ 6955 batches | lr 0.03 | ms/batch 15.09 | loss  4.39 | ppl    80.95\n| epoch  99 |  2200/ 6955 batches | lr 0.03 | ms/batch 14.97 | loss  4.31 | ppl    74.54\n| epoch  99 |  2400/ 6955 batches | lr 0.03 | ms/batch 14.84 | loss  4.31 | ppl    74.75\n| epoch  99 |  2600/ 6955 batches | lr 0.03 | ms/batch 14.91 | loss  4.34 | ppl    76.71\n| epoch  99 |  2800/ 6955 batches | lr 0.03 | ms/batch 14.92 | loss  4.38 | ppl    79.95\n| epoch  99 |  3000/ 6955 batches | lr 0.03 | ms/batch 14.88 | loss  4.42 | ppl    83.43\n| epoch  99 |  3200/ 6955 batches | lr 0.03 | ms/batch 14.84 | loss  4.34 | ppl    76.81\n| epoch  99 |  3400/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.35 | ppl    77.28\n| epoch  99 |  3600/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.44 | ppl    84.93\n| epoch  99 |  3800/ 6955 batches | lr 0.03 | ms/batch 14.96 | loss  4.39 | ppl    80.78\n| epoch  99 |  4000/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.35 | ppl    77.14\n| epoch  99 |  4200/ 6955 batches | lr 0.03 | ms/batch 15.05 | loss  4.42 | ppl    83.05\n| epoch  99 |  4400/ 6955 batches | lr 0.03 | ms/batch 15.12 | loss  4.44 | ppl    84.51\n| epoch  99 |  4600/ 6955 batches | lr 0.03 | ms/batch 15.06 | loss  4.37 | ppl    78.76\n| epoch  99 |  4800/ 6955 batches | lr 0.03 | ms/batch 15.07 | loss  4.35 | ppl    77.43\n| epoch  99 |  5000/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.23 | ppl    68.96\n| epoch  99 |  5200/ 6955 batches | lr 0.03 | ms/batch 15.12 | loss  4.25 | ppl    70.23\n| epoch  99 |  5400/ 6955 batches | lr 0.03 | ms/batch 15.09 | loss  4.31 | ppl    74.70\n| epoch  99 |  5600/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.36 | ppl    78.54\n| epoch  99 |  5800/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.32 | ppl    75.12\n| epoch  99 |  6000/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.38 | ppl    79.56\n| epoch  99 |  6200/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.35 | ppl    77.12\n| epoch  99 |  6400/ 6955 batches | lr 0.03 | ms/batch 15.02 | loss  4.29 | ppl    72.61\n| epoch  99 |  6600/ 6955 batches | lr 0.03 | ms/batch 14.90 | loss  4.32 | ppl    75.36\n| epoch  99 |  6800/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.30 | ppl    73.34\n-----------------------------------------------------------------------------------------\n| end of epoch  99 | time: 106.30s | valid loss  5.35 | valid ppl   210.25\n-----------------------------------------------------------------------------------------\n| epoch 100 |   200/ 6955 batches | lr 0.03 | ms/batch 14.80 | loss  4.39 | ppl    80.41\n| epoch 100 |   400/ 6955 batches | lr 0.03 | ms/batch 14.86 | loss  4.37 | ppl    78.92\n| epoch 100 |   600/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.43 | ppl    83.55\n| epoch 100 |   800/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.43 | ppl    84.00\n| epoch 100 |  1000/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.36 | ppl    78.17\n| epoch 100 |  1200/ 6955 batches | lr 0.03 | ms/batch 14.90 | loss  4.30 | ppl    73.89\n| epoch 100 |  1400/ 6955 batches | lr 0.03 | ms/batch 14.91 | loss  4.24 | ppl    69.07\n| epoch 100 |  1600/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.30 | ppl    73.70\n| epoch 100 |  1800/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.34 | ppl    77.06\n| epoch 100 |  2000/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.39 | ppl    80.86\n| epoch 100 |  2200/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.31 | ppl    74.57\n| epoch 100 |  2400/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.31 | ppl    74.63\n| epoch 100 |  2600/ 6955 batches | lr 0.03 | ms/batch 14.86 | loss  4.34 | ppl    76.52\n| epoch 100 |  2800/ 6955 batches | lr 0.03 | ms/batch 14.85 | loss  4.38 | ppl    79.48\n| epoch 100 |  3000/ 6955 batches | lr 0.03 | ms/batch 14.94 | loss  4.43 | ppl    83.86\n| epoch 100 |  3200/ 6955 batches | lr 0.03 | ms/batch 14.88 | loss  4.34 | ppl    77.08\n| epoch 100 |  3400/ 6955 batches | lr 0.03 | ms/batch 14.92 | loss  4.36 | ppl    77.88\n| epoch 100 |  3600/ 6955 batches | lr 0.03 | ms/batch 14.90 | loss  4.44 | ppl    84.43\n| epoch 100 |  3800/ 6955 batches | lr 0.03 | ms/batch 14.83 | loss  4.40 | ppl    81.32\n| epoch 100 |  4000/ 6955 batches | lr 0.03 | ms/batch 14.86 | loss  4.35 | ppl    77.35\n| epoch 100 |  4200/ 6955 batches | lr 0.03 | ms/batch 14.88 | loss  4.42 | ppl    82.93\n| epoch 100 |  4400/ 6955 batches | lr 0.03 | ms/batch 14.87 | loss  4.44 | ppl    84.48\n| epoch 100 |  4600/ 6955 batches | lr 0.03 | ms/batch 14.96 | loss  4.36 | ppl    78.59\n| epoch 100 |  4800/ 6955 batches | lr 0.03 | ms/batch 14.95 | loss  4.35 | ppl    77.31\n| epoch 100 |  5000/ 6955 batches | lr 0.03 | ms/batch 14.82 | loss  4.24 | ppl    69.40\n| epoch 100 |  5200/ 6955 batches | lr 0.03 | ms/batch 14.91 | loss  4.26 | ppl    70.67\n| epoch 100 |  5400/ 6955 batches | lr 0.03 | ms/batch 14.88 | loss  4.32 | ppl    75.02\n| epoch 100 |  5600/ 6955 batches | lr 0.03 | ms/batch 14.92 | loss  4.36 | ppl    78.10\n| epoch 100 |  5800/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.33 | ppl    75.64\n| epoch 100 |  6000/ 6955 batches | lr 0.03 | ms/batch 14.86 | loss  4.38 | ppl    80.17\n| epoch 100 |  6200/ 6955 batches | lr 0.03 | ms/batch 14.89 | loss  4.35 | ppl    77.69\n| epoch 100 |  6400/ 6955 batches | lr 0.03 | ms/batch 15.07 | loss  4.29 | ppl    72.82\n| epoch 100 |  6600/ 6955 batches | lr 0.03 | ms/batch 15.09 | loss  4.32 | ppl    75.12\n| epoch 100 |  6800/ 6955 batches | lr 0.03 | ms/batch 15.11 | loss  4.30 | ppl    73.53\n-----------------------------------------------------------------------------------------\n| end of epoch 100 | time: 105.49s | valid loss  5.34 | valid ppl   208.99\n-----------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "att_array = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    att = test_sample(model)\n",
    "    att_array.append(att)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "        torch.save(best_model.state_dict(),\"model/transformer1.cpt\")\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "model.load_state_dict(torch.load(\"model/transformer1.cpt\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    init_word = torch.tensor([[TEXT.vocab.stoi['what']], [TEXT.vocab.stoi['do']], [TEXT.vocab.stoi['you']], [TEXT.vocab.stoi['want']]]).to(device)\n",
    "    for i in range(20):\n",
    "        output_word = torch.argmax(model(init_word)[0], dim=2)\n",
    "        output_word = output_word[-1]\n",
    "        init_word = torch.cat([init_word, output_word.view(1,1)], dim=0)\n",
    "\n",
    "    for c in init_word:\n",
    "        print(TEXT.vocab.itos[c.data])\n",
    "        if(c.data == 3):\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([300, 10, 10])\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "att_array = torch.cat([torch.cat(a, dim=0) for a in att_array], dim=0)\n",
    "print(att_array.size())\n",
    "\n",
    "np_att_array = att_array.cpu().numpy()\n",
    "np.save(\"data/att_maps\", np_att_array)"
   ]
  }
 ]
}